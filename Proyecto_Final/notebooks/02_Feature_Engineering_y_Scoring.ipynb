{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqg7X7oJSttr9rKHj3jnck"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Implementación de Redes Neuronales**"],"metadata":{"id":"gWLsWoyezlCO"}},{"cell_type":"code","source":["# Preparar espacio de trabajo\n","\n","# Importaciones\n","import sys, os\n","from google.colab import drive\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","# Montar Google Drive\n","mount_path = '/content/drive'\n","if not os.path.ismount(mount_path):\n","    drive.mount(mount_path)\n","    print(\"Google Drive montado.\")\n","else:\n","    print(\"Google Drive ya está montado.\")\n","\n","# Crear la estructura de carpetas si no existe, siguiendo las buenas prácticas.\n","os.makedirs(\"src\", exist_ok=True)       # Código fuente\n","os.makedirs(\"notebooks\", exist_ok=True) # Cuadernos de Jupyter\n","os.makedirs(\"data\", exist_ok=True)      # Datos crudos y procesados\n","os.makedirs(\"results\", exist_ok=True)   # Gráficos y resultados de experimentos\n","os.makedirs(\"docs\", exist_ok=True)      # Documentación y reportes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwELfbZSj2f-","executionInfo":{"status":"ok","timestamp":1766019839493,"user_tz":300,"elapsed":27873,"user":{"displayName":"ANDREA KATHERINE ORDONEZ ROLDAN","userId":"01977096985746841296"}},"outputId":"ddf5eac5-a7e2-40c1-b940-6462d4ae869e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Google Drive montado.\n"]}]},{"cell_type":"markdown","source":["# **Implementación de Red Neuronal Básica**"],"metadata":{"id":"zIzBnG1IpDoO"}},{"cell_type":"code","source":["## Crear archivo\n","%%writefile src/neural_network.py\n","\n","from __future__ import annotations\n","from typing import List, Sequence, Dict, Tuple, Callable, Optional\n","import numpy as np\n","from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, accuracy_score\n","\n","# Funciones de Activación y sus Derivadas\n","def sigmoid(z: np.ndarray) -> np.ndarray:\n","    \"\"\"Implementación de la función de activación Sigmoid, estable numéricamente.\"\"\"\n","    z_clip = np.clip(z, -500, 500)\n","    return 1.0 / (1.0 + np.exp(-z_clip))\n","\n","def dsigmoid(a: np.ndarray) -> np.ndarray:\n","    \"\"\"Derivada de Sigmoid en términos de la salida 'a'.\"\"\"\n","    return a * (1.0 - a)\n","\n","def relu(z: np.ndarray) -> np.ndarray:\n","    \"\"\"Implementación de la función de activación Rectified Linear Unit (ReLU).\"\"\"\n","    return np.maximum(0.0, z)\n","\n","def drelu(z: np.ndarray, a: Optional[np.ndarray] = None) -> np.ndarray:\n","    \"\"\"Derivada de ReLU en función de 'z': 1 si z>0, 0 en caso contrario.\"\"\"\n","    g = np.zeros_like(z)\n","    g[z > 0] = 1.0\n","    return g\n","\n","def tanh(z: np.ndarray) -> np.ndarray:\n","    \"\"\"Implementación de la función de activación Tanh.\"\"\"\n","    return np.tanh(z)\n","\n","def dtanh(a: np.ndarray) -> np.ndarray:\n","    \"\"\"Derivada de Tanh en términos de la salida 'a'.\"\"\"\n","    return 1.0 - np.power(a, 2)\n","\n","# Mapa de Funciones y la Clase NeuralNetwork\n","\n","# Diccionario para mapear nombres de activación a sus funciones y derivadas.\n","_ACT_FUNCS: Dict[str, Tuple[Callable[[np.ndarray], np.ndarray], Callable[..., np.ndarray]]] = {\n","    \"sigmoid\": (sigmoid, dsigmoid),\n","    \"relu\": (relu, drelu),\n","    \"tanh\": (tanh, dtanh),\n","}\n","\n","# Funciones de inicialización de pesos para evitar el vanishing/exploding gradient.\n","def _he_std(fan_in: int) -> float:\n","    \"\"\"Inicialización He para ReLU: N(0, sqrt(2/fan_in)).\"\"\"\n","    return np.sqrt(2.0 / fan_in)\n","\n","def _xavier_std(fan_in: int) -> float:\n","    \"\"\"Inicialización Xavier/Glorot para sigmoid/tanh: N(0, sqrt(1/fan_in)).\"\"\"\n","    return 1.0 / np.sqrt(fan_in)\n","\n","class NeuralNetwork:\n","    \"\"\"\n","    Red neuronal totalmente conectada (MLP) implementada desde cero con NumPy.\n","    Soporta:\n","    - Múltiples capas ocultas.\n","    - Activaciones configurables por capa (Sigmoid, ReLU, Tanh).\n","    - Entrenamiento por gradiente descendente (full-batch).\n","    - Pérdida MSE para la regla delta.\n","    - Métricas de evaluación para clasificación binaria.\n","    \"\"\"\n","    def __init__(self, layers: Sequence[int], activations: Optional[Sequence[str]] = None, seed: Optional[int] = 42):\n","        assert len(layers) >= 3, \"La arquitectura debe tener al menos una capa de entrada, una oculta y una de salida.\"\n","        self.layers = list(layers)\n","        self.L = len(layers) - 1\n","\n","        if activations is None:\n","            self.activations = [\"relu\"] * (self.L - 1) + [\"sigmoid\"]\n","        else:\n","            assert len(activations) == self.L, \"El número de activaciones debe ser igual al número de capas con pesos (len(layers) - 1).\"\n","            self.activations = [a.lower() for a in activations]\n","\n","        for a in self.activations:\n","            assert a in _ACT_FUNCS, f\"Activación desconocida: {a}\"\n","\n","        self.rng = np.random.default_rng(seed)\n","        self.W: List[np.ndarray] = []\n","        self.b: List[np.ndarray] = []\n","        self._init_params()\n","\n","    def _init_params(self) -> None:\n","        \"\"\"\n","        Inicializa los pesos (W) y sesgos (b) de la red.\n","        Usa la inicialización de He para ReLU y Xavier para otras activaciones.\n","        \"\"\"\n","        self.W.clear()\n","        self.b.clear()\n","        for l in range(self.L):\n","            fan_in = self.layers[l]\n","            fan_out = self.layers[l+1]\n","            act = self.activations[l]\n","\n","            if act == \"relu\":\n","                std = _he_std(fan_in)\n","            else:\n","                std = _xavier_std(fan_in)\n","\n","            W_l = self.rng.normal(loc=0.0, scale=std, size=(fan_in, fan_out)).astype(np.float64)\n","            b_l = np.zeros((1, fan_out), dtype=np.float64)\n","            self.W.append(W_l)\n","            self.b.append(b_l)\n","\n","    def forward(self, X: np.ndarray) -> Tuple[np.ndarray, Dict[str, List[np.ndarray]]]:\n","        \"\"\"\n","        Propagación hacia adelante: calcula las salidas de cada capa.\n","        Retorna: la predicción final y un caché de los valores 'Z' y 'A' para el backprop.\n","        \"\"\"\n","        A = X\n","        A_list = [A]\n","        Z_list = []\n","\n","        for l in range(self.L):\n","            Z = A @ self.W[l] + self.b[l]\n","            act, _ = _ACT_FUNCS[self.activations[l]]\n","            A = act(Z)\n","            Z_list.append(Z)\n","            A_list.append(A)\n","\n","        cache = {\"A\": A_list, \"Z\": Z_list}\n","        return A, cache\n","\n","    def _loss_mse(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n","        \"\"\"Calcula la pérdida de error cuadrático medio (MSE).\"\"\"\n","        return float(np.mean(np.square(y_pred - y_true)))\n","\n","    def _dA_mse(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n","        \"\"\"Calcula el gradiente inicial de la pérdida con respecto a la salida.\"\"\"\n","        m = y_true.shape[0]\n","        return (2.0 / m) * (y_pred - y_true)\n","\n","    def backward(self, cache: Dict[str, List[np.ndarray]], y_true: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n","        \"\"\"\n","        Backpropagation: calcula los gradientes (dW y db) para todas las capas.\n","        Utiliza la regla delta para propagar el error hacia atrás.\n","        \"\"\"\n","        A_list = cache[\"A\"]\n","        Z_list = cache[\"Z\"]\n","\n","        dW_list = [None] * self.L\n","        db_list = [None] * self.L\n","\n","        # Gradiente inicial en la capa de salida\n","        dA = self._dA_mse(A_list[-1], y_true)\n","\n","        # Bucle para propagar hacia atrás (de la última capa a la primera)\n","        for l in reversed(range(self.L)):\n","            A_prev = A_list[l]\n","            Z = Z_list[l]\n","            act_name = self.activations[l]\n","\n","            _, dact = _ACT_FUNCS[act_name]\n","\n","            # Cálculo del gradiente local de la activación\n","            if act_name == \"relu\":\n","                dZ = dA * dact(z=Z)\n","            else:\n","                A_curr = A_list[l + 1]\n","                dZ = dA * dact(a=A_curr)\n","\n","            # Cálculo de los gradientes de pesos y sesgos\n","            dW = A_prev.T @ dZ\n","            db = np.sum(dZ, axis=0, keepdims=True)\n","\n","            # Propagación del gradiente a la capa anterior\n","            dA = dZ @ self.W[l].T\n","\n","            dW_list[l] = dW\n","            db_list[l] = db\n","\n","        return dW_list, db_list\n","\n","    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000, learning_rate: float = 0.01, verbose: bool = True) -> List[float]:\n","        \"\"\"\n","        Entrena la red neuronal usando el algoritmo de descenso de gradiente (full-batch).\n","        Retorna la lista de pérdidas por cada época.\n","        \"\"\"\n","        history = []\n","        for ep in range(1, epochs + 1):\n","            y_pred, cache = self.forward(X)\n","            loss = self._loss_mse(y_pred, y)\n","            dW_list, db_list = self.backward(cache, y)\n","\n","            # Actualizar los pesos y sesgos\n","            for l in range(self.L):\n","                self.W[l] -= learning_rate * dW_list[l]\n","                self.b[l] -= learning_rate * db_list[l]\n","\n","            history.append(loss)\n","            if verbose and (ep % max(1, epochs // 10) == 0 or ep == 1):\n","                print(f\"[{ep:4d}/{epochs}] loss={loss:.6f}\")\n","\n","        return history\n","\n","    def predict(self, X: np.ndarray) -> np.ndarray:\n","        \"\"\"Realiza una predicción hacia adelante en la red.\"\"\"\n","        y_pred, _ = self.forward(X)\n","        return y_pred\n","\n","    def accuracy(self, X: np.ndarray, y: np.ndarray, threshold: float = 0.5) -> float:\n","        \"\"\"Calcula la precisión (accuracy) binaria del modelo.\"\"\"\n","        y_hat = self.predict(X)\n","        y_bin = (y_hat >= threshold).astype(np.float64)\n","        return float(np.mean(y_bin == y))\n","\n","    def get_performance_metrics(self, X: np.ndarray, y_true: np.ndarray, threshold: float = 0.5) -> dict:\n","        \"\"\"\n","        Calcula las métricas de rendimiento clave (precisión, recall, F1-Score, matriz de confusión)\n","        para una clasificación binaria.\n","        \"\"\"\n","        y_pred_proba = self.predict(X)\n","        y_pred_binary = (y_pred_proba >= threshold).astype(np.int64)\n","        y_true = y_true.ravel()\n","\n","        cm = confusion_matrix(y_true, y_pred_binary)\n","        # Asegurarse de que la matriz de confusión tiene 4 elementos (2x2)\n","        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n","\n","        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n","\n","        f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n","        accuracy = accuracy_score(y_true, y_pred_binary)\n","        precision = precision_score(y_true, y_pred_binary, zero_division=0)\n","\n","        return {\n","            'accuracy': accuracy,\n","            'precision': precision,\n","            'recall': sensitivity,\n","            'specificity': specificity,\n","            'f1_score': f1,\n","            'confusion_matrix': cm\n","        }\n","\n","    def threshold_optimization(self, X: np.ndarray, y_true: np.ndarray) -> tuple:\n","        \"\"\"\n","        Encuentra el umbral óptimo que maximiza el F1-Score en el conjunto de datos.\n","        Retorna el umbral óptimo y el diccionario de métricas correspondiente.\n","        \"\"\"\n","        y_pred_proba = self.predict(X)\n","        thresholds = np.linspace(0, 1, 101) # Probar 101 umbrales de 0 a 1\n","        best_f1 = -1\n","        best_threshold = 0.5\n","        best_metrics = {}\n","\n","        for threshold in thresholds:\n","            metrics = self.get_performance_metrics(X, y_true, threshold)\n","            if metrics['f1_score'] > best_f1:\n","                best_f1 = metrics['f1_score']\n","                best_threshold = threshold\n","                best_metrics = metrics\n","\n","        return best_threshold, best_metrics\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwGmfGJNkAXd","executionInfo":{"status":"ok","timestamp":1766020020848,"user_tz":300,"elapsed":60,"user":{"displayName":"ANDREA KATHERINE ORDONEZ ROLDAN","userId":"01977096985746841296"}},"outputId":"8064cbf5-87d5-4cd6-dbb5-00a11e81ccda"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing src/neural_network.py\n"]}]},{"cell_type":"markdown","source":["# **Pipeline de feature engineering**"],"metadata":{"id":"5gKF7eV_k2FG"}},{"cell_type":"code","source":["# Crear archivo de preprocesamiento\n","%%writefile src/data_preprocessing.py\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from typing import List, Tuple\n","\n","def preprocess_data(df: pd.DataFrame, num_cols: List[str], cat_cols: List[str]) -> Tuple[np.ndarray, np.ndarray, StandardScaler, OneHotEncoder]:\n","    \"\"\"\n","    Preprocesa un DataFrame para ser usado en la red neuronal.\n","    Aplica:\n","    - Estandarización a columnas numéricas.\n","    - One-Hot Encoding a columnas categóricas.\n","    - Combina las características en una matriz de NumPy.\n","    - Separa las etiquetas (fraude).\n","    \"\"\"\n","    if df.empty:\n","        raise ValueError(\"El DataFrame de entrada no puede estar vacío.\")\n","\n","    scaler = StandardScaler()\n","    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n","\n","    # Preprocesar columnas numéricas\n","    X_num = scaler.fit_transform(df[num_cols])\n","\n","    # Preprocesar columnas categóricas\n","    X_cat = encoder.fit_transform(df[cat_cols])\n","\n","    # Combinar todas las características\n","    X = np.hstack([X_num, X_cat])\n","    y = df['fraude'].values.reshape(-1, 1)\n","\n","    return X, y, scaler, encoder"],"metadata":{"id":"lA0KEJ3wkKXk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766020078876,"user_tz":300,"elapsed":34,"user":{"displayName":"ANDREA KATHERINE ORDONEZ ROLDAN","userId":"01977096985746841296"}},"outputId":"3507271d-e1e1-4348-c107-164fce97e607"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing src/data_preprocessing.py\n"]}]},{"cell_type":"markdown","source":["# **Aplicación del modelo al problema de fraude**"],"metadata":{"id":"bZsQqh-qlWv2"}},{"cell_type":"code","source":["import sys\n","import os\n","\n","# Añadir el directorio 'src' al path para poder importar los módulos\n","if os.getcwd() not in sys.path:\n","    sys.path.insert(0, os.getcwd())\n","if os.path.join(os.getcwd(), 'src') not in sys.path:\n","    sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n","\n","# Detección de Fraudes\n","\n","# Importamos los módulos que acabamos de crear.\n","from data_preprocessing import preprocess_data\n","from neural_network import NeuralNetwork # Importar NeuralNetwork también\n","\n","# Cargar los datos sintéticos de la carpeta 'data'.\n","df = pd.read_csv('/content/data/transacciones_fraude_sinteticas.csv')\n","\n","# Renombrar la columna 'is_fraud' a 'fraude' para que coincida con la expectativa de preprocess_data.\n","# Corregido el error lógico en la condición:\n","if 'is_fraud' in df.columns and 'fraude' not in df.columns:\n","    df = df.rename(columns={'is_fraud': 'fraude'})\n","\n","# Definir las columnas numéricas y categóricas.\n","# Corregido: 'aomunt' a 'amount' y 'conutry' a 'country'\n","num_cols = ['amount', 'hour', 'high_amount', 'avg_amount', 'is_foreign']\n","cat_cols = ['country', 'channel', 'device','merchant_category']\n","\n","# Preprocesar los datos para la red neuronal.\n","X, y, _, _ = preprocess_data(df, num_cols, cat_cols)\n","\n","# Dividir el dataset en conjuntos de entrenamiento y prueba, manteniendo la proporción de la clase 'fraude'.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n","\n","# Entrenar la red neuronal para el problema de fraude\n","# La arquitectura se ajusta al número de características preprocesadas (X.shape[1]).\n","nn = NeuralNetwork(layers=[X.shape[1], 32, 16, 1], activations=['tanh', 'tanh', 'sigmoid'])\n","history = nn.train(X_train, y_train, epochs=300, learning_rate=0.05, verbose=True)\n","\n","# Evaluación del modelo con un umbral estándar de 0.5.\n","print(\"\\n--- Evaluación con Umbral = 0.5 ---\")\n","metrics_standard = nn.get_performance_metrics(X_test, y_test, threshold=0.5)\n","print(f\"Accuracy: {metrics_standard['accuracy']:.4f}\")\n","print(f\"Precision: {metrics_standard['precision']:.4f}\")\n","print(f\"Recall (Sensibilidad): {metrics_standard['recall']:.4f}\")\n","print(f\"F1-Score: {metrics_standard['f1_score']:.4f}\")\n","print(\"Matriz de Confusión:\\n\", metrics_standard['confusion_matrix'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9_IeVQMlTiu","executionInfo":{"status":"ok","timestamp":1766020578904,"user_tz":300,"elapsed":347541,"user":{"displayName":"ANDREA KATHERINE ORDONEZ ROLDAN","userId":"01977096985746841296"}},"outputId":"4742ca76-0dfb-4848-b261-17415f3c3e6d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[   1/300] loss=0.271756\n","[  30/300] loss=0.058873\n","[  60/300] loss=0.023754\n","[  90/300] loss=0.013797\n","[ 120/300] loss=0.009540\n","[ 150/300] loss=0.007271\n","[ 180/300] loss=0.005890\n","[ 210/300] loss=0.004973\n","[ 240/300] loss=0.004325\n","[ 270/300] loss=0.003846\n","[ 300/300] loss=0.003478\n","\n","--- Evaluación con Umbral = 0.5 ---\n","Accuracy: 0.9990\n","Precision: 0.0000\n","Recall (Sensibilidad): 0.0000\n","F1-Score: 0.0000\n","Matriz de Confusión:\n"," [[39960     0]\n"," [   40     0]]\n"]}]},{"cell_type":"code","source":["#Implementar threshold optimization\n","# Optimización del umbral y evaluación con métricas óptimas\n","\n","# En problemas de fraude, el F1-Score es más relevante que la precisión.\n","print(\"\\n--- Optimización del Umbral ---\")\n","best_threshold, best_metrics = nn.threshold_optimization(X_test, y_test)\n","\n","# Imprimir las mejores métricas encontradas.\n","print(f\"Umbral óptimo: {best_threshold:.4f}\")\n","print(\"Mejores métricas:\")\n","print(f\"Accuracy: {best_metrics['accuracy']:.4f}\")\n","print(f\"Precision: {best_metrics['precision']:.4f}\")\n","print(f\"Recall (Sensibilidad): {best_metrics['recall']:.4f}\")\n","print(f\"F1-Score: {best_metrics['f1_score']:.4f}\")\n","print(\"Matriz de Confusión Óptima:\\n\", best_metrics['confusion_matrix'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPWLuLUyp2SR","executionInfo":{"status":"ok","timestamp":1766020742207,"user_tz":300,"elapsed":17629,"user":{"displayName":"ANDREA KATHERINE ORDONEZ ROLDAN","userId":"01977096985746841296"}},"outputId":"e3b4cc0d-b4fa-4575-d768-c15459ca378e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Optimización del Umbral ---\n","Umbral óptimo: 0.1200\n","Mejores métricas:\n","Accuracy: 0.9992\n","Precision: 0.7857\n","Recall (Sensibilidad): 0.2750\n","F1-Score: 0.4074\n","Matriz de Confusión Óptima:\n"," [[39957     3]\n"," [   29    11]]\n"]}]},{"cell_type":"markdown","source":["# **Experimentación Comparativa**"],"metadata":{"id":"zHuXCqisqKhv"}},{"cell_type":"code","source":["#=====================================\n","#Comparación de Arquitecturas: Pruebe al menos 3 configuraciones diferentes de capas ocultas\n","#=====================================\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","#Escalado de variables\n","scaler = StandardScaler()\n","X_train_s = scaler.fit_transform(X_train)\n","X_test_s = scaler.transform(X_test)\n","\n","\n","def build_model(hidden_layers, activation=\"relu\", lr=0.001):\n","    model = Sequential()\n","    model.add(Dense(hidden_layers[0], activation=activation, input_shape=(X_train_s.shape[1],)))\n","\n","    for units in hidden_layers[1:]:\n","        model.add(Dense(units, activation=activation))\n","        model.add(Dropout(0.2))\n","\n","    model.add(Dense(1, activation=\"sigmoid\"))\n","\n","    model.compile(\n","        optimizer=Adam(learning_rate=lr),\n","        loss=\"binary_crossentropy\",\n","        metrics=[\"AUC\"]\n","    )\n","    return model\n","\n","#Definir arquitecturas\n","\n","architectures = {\n","    \"Simple\": [16],\n","    \"Medium\": [32, 16],\n","    \"Deep\": [64, 32, 16]\n","}\n","\n","from sklearn.metrics import roc_auc_score\n","\n","results_arch = {}\n","\n","for name, layers in architectures.items():\n","    model = build_model(layers)\n","\n","    model.fit(\n","        X_train, y_train,\n","        epochs=10,\n","        batch_size=256,\n","        validation_split=0.2,\n","        class_weight={0:1, 1:10},\n","        verbose=0\n","    )\n","\n","    y_pred = model.predict(X_test).ravel()\n","    auc = roc_auc_score(y_test, y_pred)\n","    results_arch[name] = auc\n","\n","results_arch\n","\n","# Convertir el diccionario de resultados en un DataFrame\n","results_df = pd.DataFrame(results_arch.items(), columns=['Arquitectura', 'AUC_Score'])\n","\n","# Establecer la 'Arquitectura' como índice para una mejor presentación\n","results_df = results_df.set_index('Arquitectura')\n","print(\"=\"*30)\n","print(\"Comparación de arquitecturas\")\n","print(\"=\"*30)\n","# Mostrar la tabla\n","print(results_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzc4kOUuqKZS","executionInfo":{"status":"ok","timestamp":1766021025462,"user_tz":300,"elapsed":77753,"user":{"displayName":"ANDREA KATHERINE ORDONEZ ROLDAN","userId":"01977096985746841296"}},"outputId":"17eb9d38-5502-4bc1-e3b9-819908af8d5c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:67: SyntaxWarning: invalid escape sequence '\\C'\n","<>:67: SyntaxWarning: invalid escape sequence '\\C'\n","/tmp/ipython-input-1635167874.py:67: SyntaxWarning: invalid escape sequence '\\C'\n","  print(\"\\Comparación de arquitecturas\")\n","/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n","==============================\n","\\Comparación de arquitecturas\n","==============================\n","              AUC_Score\n","Arquitectura           \n","Simple         0.998509\n","Medium         0.998345\n","Deep           0.998005\n"]}]},{"cell_type":"code","source":["#==================================================================================================\n","#Análisis de Funciones de Activación: Compare rendimiento con diferentes activaciones\n","#==================================================================================================\n","activations = [\"relu\", \"tanh\", \"elu\"]\n","results_act = {}\n","\n","for act in activations:\n","    model = build_model([32,16], activation=act)\n","\n","    model.fit(\n","        X_train_s, y_train,\n","        epochs=10,\n","        batch_size=256,\n","        validation_split=0.2,\n","        class_weight={0:1, 1:10},\n","        verbose=0\n","    )\n","\n","    y_pred = model.predict(X_test_s).ravel()\n","    results_act[act] = roc_auc_score(y_test, y_pred)\n","\n","results_act\n","\n","# Convertir el diccionario de resultados en un DataFrame\n","results_ac = pd.DataFrame(results_act.items(), columns=['Funciones', 'Rendimientos'])\n","\n","# Establecer la 'Arquitectura' como índice para una mejor presentación\n","results_ac = results_ac.set_index('Funciones')\n","print(\"=\"*30)\n","print(\"Comparación de arquitecturas\")\n","print(\"=\"*30)\n","\n","# Mostrar la tabla\n","print(results_ac)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XriXdt8Pu2ox","executionInfo":{"status":"ok","timestamp":1766021310444,"user_tz":300,"elapsed":69277,"user":{"displayName":"ANDREA KATHERINE ORDONEZ ROLDAN","userId":"01977096985746841296"}},"outputId":"cc4c0692-d642-492e-fb56-9b915ab1da28"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","==============================\n","Comparación de arquitecturas\n","==============================\n","           Rendimientos\n","Funciones              \n","relu           0.997839\n","tanh           0.996935\n","elu            0.997920\n"]}]},{"cell_type":"code","source":["#=================================\n","#Optimización de Hiperparámetros\n","#==================================\n","learning_rates = [0.001, 0.0005]\n","epochs_list = [10, 20]\n","\n","results_hyper = {}\n","\n","for lr in learning_rates:\n","    for ep in epochs_list:\n","        model = build_model([32,16], lr=lr)\n","\n","        model.fit(\n","            X_train_s, y_train,\n","            epochs=ep,\n","            batch_size=256,\n","            validation_split=0.2,\n","            class_weight={0:1, 1:10},\n","            verbose=0\n","        )\n","\n","        y_pred = model.predict(X_test_s).ravel()\n","        results_hyper[f\"lr={lr}_ep={ep}\"] = roc_auc_score(y_test, y_pred)\n","\n","results_hyper\n","\n","# Convertir el diccionario de resultados en un DataFrame\n","results_hyp = pd.DataFrame(results_hyper.items(), columns=['Hiperparámetros', 'Valor'])\n","\n","# Establecer la 'Arquitectura' como índice para una mejor presentación\n","results_hyp = results_hyp.set_index('Hiperparámetros')\n","print(\"=\"*30)\n","print(\"Optimización de Hiperparámetros\")\n","print(\"=\"*30)\n","# Mostrar la tabla\n","print(results_hyp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiRv43UYvh1h","executionInfo":{"status":"ok","timestamp":1766021639398,"user_tz":300,"elapsed":129917,"user":{"displayName":"ANDREA KATHERINE ORDONEZ ROLDAN","userId":"01977096985746841296"}},"outputId":"0db469a9-55a5-4b22-dc72-6e02e654a925"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n","==============================\n","Optimización de Hiperparámetros\n","==============================\n","                    Valor\n","Hiperparámetros          \n","lr=0.001_ep=10   0.998549\n","lr=0.001_ep=20   0.997197\n","lr=0.0005_ep=10  0.998131\n","lr=0.0005_ep=20  0.998172\n"]}]},{"cell_type":"code","source":["#=========================\n","#Baseline Comparison\n","#=========================\n","from sklearn.linear_model import LogisticRegression\n","\n","log_model = LogisticRegression(\n","    max_iter=1000,\n","    class_weight={0:1, 1:10}\n",")\n","\n","log_model.fit(X_train_s, y_train)\n","\n","y_pred_log = log_model.predict_proba(X_test_s)[:,1]\n","auc_log = roc_auc_score(y_test, y_pred_log)\n","\n","auc_log\n","print(\"=\"*30)\n","print(\"****Baseline Comparison****\")\n","print(\"=\"*30)\n","print(\"AUC=\",f\"{auc_log * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZuanjJ0wIX5","executionInfo":{"status":"ok","timestamp":1766021888783,"user_tz":300,"elapsed":1513,"user":{"displayName":"ANDREA KATHERINE ORDONEZ ROLDAN","userId":"01977096985746841296"}},"outputId":"53ce8ead-fefc-4abb-93f2-55a0a7695e08"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["==============================\n","****Baseline Comparison****\n","==============================\n","AUC= 99.87%\n"]}]},{"cell_type":"code","source":["# ================================================\n","#  CELDA FINAL: GUARDAR NOTEBOOK EN GITHUB DESDE COLAB\n","# ================================================\n","import os\n","import json\n","import subprocess\n","from google.colab import drive\n","\n","# CONFIGURACIÓN - Ajusta estos valores para cada notebook\n","NOTEBOOK_NAME = \"02_Feature_Engineering_y_Scoring.ipynb\"  # Cambia por el nombre actual\n","COMMIT_MESSAGE = \"Actualizar notebook de proyecto\"  # Cambia el mensaje\n","GITHUB_USERNAME = \"akordone22\"\n","REPO_NAME = \"UEES-IA-Deteccion_Fraude-Grupo7\"\n","\n","# Token de GitHub - CONFIGURADO DIRECTAMENTE\n","GITHUB_TOKEN = \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\"\n","\n","def get_github_token():\n","    \"\"\"Retorna el token de GitHub configurado\"\"\"\n","    return GITHUB_TOKEN\n","\n","def mount_drive():\n","    \"\"\"Monta Google Drive si no está montado\"\"\"\n","    try:\n","        if not os.path.exists('/content/drive'):\n","            print(\"Montando Google Drive...\")\n","            drive.mount('/content/drive')\n","        print(\"EXITO: Google Drive montado correctamente\")\n","        return True\n","    except Exception as e:\n","        print(f\"ERROR: Error montando Google Drive: {e}\")\n","        return False\n","\n","def configure_git():\n","    \"\"\"Configura Git con credenciales\"\"\"\n","    try:\n","        # Primero intentar configurar de forma local en el directorio\n","        os.makedirs('/tmp/git_config', exist_ok=True)\n","        os.chdir('/tmp/git_config')\n","\n","        # Inicializar git temporalmente\n","        subprocess.run(['git', 'init'], check=True, capture_output=True)\n","\n","        # Configurar usuario\n","        result1 = subprocess.run(['git', 'config', 'user.email', 'andrea.ordonezr@uees.edu.ec'],\n","                                capture_output=True, text=True)\n","        result2 = subprocess.run(['git', 'config', 'user.name', 'Andrea Ordoñez'],\n","                                capture_output=True, text=True)\n","\n","        # Verificar configuración\n","        email_check = subprocess.run(['git', 'config', 'user.email'],\n","                                   capture_output=True, text=True)\n","        name_check = subprocess.run(['git', 'config', 'user.name'],\n","                                  capture_output=True, text=True)\n","\n","        if 'andrea.ordonezr@uees.edu.ec' in email_check.stdout and 'Andrea Ordoñez' in name_check.stdout:\n","            print(\"EXITO: Git configurado correctamente\")\n","            return True\n","        else:\n","            # Intentar configuración global alternativa\n","            subprocess.run(['git', 'config', '--global', 'user.email', 'andrea.ordonezr@uees.edu.ec'],\n","                          check=False)\n","            subprocess.run(['git', 'config', '--global', 'user.name', 'Andrea Ordoñez'],\n","                          check=False)\n","            print(\"EXITO: Git configurado (modo alternativo)\")\n","            return True\n","\n","    except subprocess.CalledProcessError as e:\n","        print(f\"ADVERTENCIA: Error configurando Git globalmente, continuando: {e}\")\n","        # Continuamos de todas formas, Git puede funcionar sin configuración global\n","        return True\n","    except Exception as e:\n","        print(f\"ERROR: Error configurando Git: {e}\")\n","        return False\n","\n","def clean_directory():\n","    \"\"\"Limpia y prepara el directorio de trabajo\"\"\"\n","    try:\n","        if os.path.exists('/content/repo_final'):\n","            subprocess.run(['rm', '-rf', '/content/repo_final'], check=True)\n","        print(\"EXITO: Directorio limpiado\")\n","        return True\n","    except Exception as e:\n","        print(f\"ERROR: Error limpiando directorio: {e}\")\n","        return False\n","\n","def clone_repository():\n","    \"\"\"Clona el repositorio de GitHub\"\"\"\n","    try:\n","        token = get_github_token()\n","        repo_url = f\"https://{GITHUB_USERNAME}:{token}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n","\n","        # Configurar Git en el directorio del repositorio después de clonar\n","        result = subprocess.run(['git', 'clone', repo_url, '/content/repo_final'],\n","                               capture_output=True, text=True, check=True)\n","\n","        # Cambiar al directorio del repositorio y configurar usuario\n","        os.chdir('/content/repo_final')\n","        subprocess.run(['git', 'config', 'user.email', 'andrea.ordonezr@uees.edu.ec'], check=False)\n","        subprocess.run(['git', 'config', 'user.name', 'Andrea Ordoñez'], check=False)\n","\n","        print(\"EXITO: Repositorio clonado y configurado correctamente\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"ERROR: Error clonando repositorio: {e}\")\n","        print(f\"STDOUT: {e.stdout}\")\n","        print(f\"STDERR: {e.stderr}\")\n","        return False\n","\n","def clean_notebook_content(content):\n","    \"\"\"Limpia el contenido del notebook removiendo tokens sensibles\"\"\"\n","    # Lista de patrones a remover (incluyendo el token actual)\n","    token = get_github_token()\n","    sensitive_patterns = [\n","        token,\n","        \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\",\n","        \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\",\n","        \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\",\n","        \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\",\n","        \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\",\n","        \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\"\n","    ]\n","\n","    cleaned_content = content\n","    for pattern in sensitive_patterns:\n","        if pattern and pattern in cleaned_content:\n","            cleaned_content = cleaned_content.replace(pattern, \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\")\n","\n","    # Limpieza adicional: remover líneas que contengan tokens\n","    lines = cleaned_content.split('\\n')\n","    clean_lines = []\n","    for line in lines:\n","        # Si la línea contiene algún patrón de token, la reemplazamos\n","        line_has_token = False\n","        for pattern in [\"[TOKEN_REMOVIDO_POR_SEGURIDAD]\", \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\", \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\", \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\", \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\", \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\"]:\n","            if pattern in line:\n","                line_has_token = True\n","                break\n","\n","        if line_has_token:\n","            # Reemplazar toda la línea si contiene un token\n","            if \"GITHUB_TOKEN\" in line:\n","                clean_lines.append('GITHUB_TOKEN = \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\"')\n","            else:\n","                clean_lines.append(\"[LINEA_CON_TOKEN_REMOVIDA_POR_SEGURIDAD]\")\n","        else:\n","            clean_lines.append(line)\n","\n","    return '\\n'.join(clean_lines)\n","\n","def copy_and_clean_notebook():\n","    \"\"\"Copia el notebook desde Drive y lo limpia\"\"\"\n","    try:\n","        # Rutas\n","        drive_path = f\"/content/drive/MyDrive/proyectof/{NOTEBOOK_NAME}\"\n","        target_dir = \"/content/repo_final/Proyecto_Final/notebooks\"\n","        target_path = f\"{target_dir}/{NOTEBOOK_NAME}\"\n","\n","        # Verificar que el notebook existe en Drive\n","        if not os.path.exists(drive_path):\n","            print(f\"ERROR: No se encontró el notebook en: {drive_path}\")\n","            return False\n","\n","        # Crear directorio destino\n","        os.makedirs(target_dir, exist_ok=True)\n","\n","        # Leer el notebook\n","        with open(drive_path, 'r', encoding='utf-8') as f:\n","            content = f.read()\n","\n","        # Limpiar contenido sensible\n","        cleaned_content = clean_notebook_content(content)\n","\n","        # Verificar que el token fue removido completamente\n","        token = get_github_token()\n","        if token in cleaned_content:\n","            print(\"ADVERTENCIA: Token detectado en contenido después de limpieza\")\n","            # Limpieza adicional más agresiva\n","            cleaned_content = cleaned_content.replace(token, \"[TOKEN_REMOVIDO_POR_SEGURIDAD]\")\n","\n","        # Guardar el notebook limpio\n","        with open(target_path, 'w', encoding='utf-8') as f:\n","            f.write(cleaned_content)\n","\n","        print(f\"EXITO: Notebook '{NOTEBOOK_NAME}' copiado y limpiado\")\n","        print(\"SEGURIDAD: Contenido verificado - tokens removidos\")\n","        return True\n","\n","    except Exception as e:\n","        print(f\"ERROR: Error copiando notebook: {e}\")\n","        return False\n","\n","def commit_and_push():\n","    \"\"\"Hace commit y push de los cambios\"\"\"\n","    try:\n","        # Cambiar al directorio del repositorio\n","        os.chdir('/content/repo_final')\n","\n","        # Verificar estado\n","        result = subprocess.run(['git', 'status', '--porcelain'],\n","                               capture_output=True, text=True, check=True)\n","\n","        if not result.stdout.strip():\n","            print(\"EXITO: No hay cambios para subir\")\n","            return True\n","\n","        # Verificación final de seguridad antes de subir\n","        token = get_github_token()\n","        for root, dirs, files in os.walk('.'):\n","            for file in files:\n","                if file.endswith(('.ipynb', '.py', '.md', '.txt')):\n","                    filepath = os.path.join(root, file)\n","                    try:\n","                        with open(filepath, 'r', encoding='utf-8') as f:\n","                            content = f.read()\n","                            if token in content:\n","                                print(f\"PELIGRO: Token detectado en {filepath}\")\n","                                print(\"DETENIENDO PROCESO POR SEGURIDAD\")\n","                                return False\n","                    except:\n","                        pass  # Ignorar archivos que no se pueden leer\n","\n","        print(\"SEGURIDAD: Verificación final completada - sin tokens detectados\")\n","\n","        # Agregar archivos\n","        subprocess.run(['git', 'add', '.'], check=True)\n","\n","        # Commit\n","        subprocess.run(['git', 'commit', '-m', COMMIT_MESSAGE], check=True)\n","\n","        # Push\n","        subprocess.run(['git', 'push', 'origin', 'main'], check=True)\n","\n","        print(\"EXITO: Cambios subidos exitosamente a GitHub\")\n","        return True\n","\n","    except subprocess.CalledProcessError as e:\n","        print(f\"ERROR: Error en commit/push: {e}\")\n","        if e.stderr:\n","            print(f\"STDERR: {e.stderr}\")\n","        return False\n","\n","def main():\n","    \"\"\"Función principal que ejecuta todo el proceso\"\"\"\n","    print(\"=== INICIANDO PROCESO DE SUBIDA A GITHUB ===\")\n","    print(\"NOTA: Token configurado directamente en el código\")\n","    print(\"\")\n","\n","    steps = [\n","        (\"Montando Google Drive\", mount_drive),\n","        (\"Configurando Git\", configure_git),\n","        (\"Limpiando directorio\", clean_directory),\n","        (\"Clonando repositorio\", clone_repository),\n","        (\"Copiando y limpiando notebook\", copy_and_clean_notebook),\n","        (\"Subiendo cambios\", commit_and_push)\n","    ]\n","\n","    for step_name, step_function in steps:\n","        print(f\"\\n{step_name}...\")\n","        if not step_function():\n","            print(f\"\\nERROR: PROCESO FALLIDO en: {step_name}\")\n","            return False\n","\n","    print(f\"\\nEXITO: PROCESO COMPLETADO EXITOSAMENTE\")\n","    print(f\"Notebook disponible en:\")\n","    print(f\"https://github.com/{GITHUB_USERNAME}/{REPO_NAME}/tree/main/Proyecto_Final/notebooks\")\n","\n","    return True\n","\n","# Ejecutar el proceso\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"k2rzMCPrHPya"},"execution_count":null,"outputs":[]}]}